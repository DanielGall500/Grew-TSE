import ast
from transformers import AutoModelForMaskedLM, AutoModelForCausalLM, AutoTokenizer
from transformers import PreTrainedModel, PreTrainedTokenizerBase
from pathlib import Path
from typing import Tuple, NamedTuple, List, Any
from contextlib import contextmanager
import torch.nn.functional as F
from tqdm import tqdm
import pandas as pd
import itertools
import logging
import torch
import math

from grewtse.utils.validation import load_and_validate_mp_dataset
from grewtse.evaluators.metrics import (
    compute_normalised_surprisal_difference,
    compute_average_surprisal_difference,
    compute_entropy_based_certainty,
    compute_accuracy,
    compute_surprisal,
    compute_mean,
)

EVAL_TEMPLATE = {
}

# --- Helper function ---
def _init_row_results(row):
    row_results = EVAL_TEMPLATE.copy()
    row_results.update(row._asdict())
    return row_results


class TooManyMasksException(Exception):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"TMM Exception: {message}")


class Prediction(NamedTuple):
    token: str
    prob: float
    surprisal: float


class GrewTSEvaluator:
    """
    An evaluation class designed specifically for rapid syntactic evaluation of models available on the Hugging Face platform.
    """

    def __init__(self):
        self.evaluator = Evaluator()
        self.evaluation_dataset = None

    def evaluate_model(
        self,
        mp_dataset: pd.DataFrame,
        model_repo: str,
        model_type: str,  # can be 'encoder' or 'decoder'
        evaluation_cols:list[str]=["form_grammatical","form_ungrammatical"],
        entropy_topk: int = 100,
        save_to: str|None=None,
        row_limit: int|None = None,
    ) -> pd.DataFrame:
        """
        Function for carrying out Targeted Syntactic Evaluation for either encoder or decoder models.

        :param mp_dataset: A DataFrame containing the Minimal-Pair Dataset generated by Grew-TSE.
        :param model_repo: the Hugging Face model repository link.
        :param model_type: choose either 'encoder' or 'decoder'.
        :param evaluation_cols: a list of strings indicating the columns containing the target words / tokens. Defaults to `form_grammatical` and `form_ungrammatical`, as used by GrewTSEPipe.
        :param entropy_topk: how many probabilities are taken into account for calculating the model uncertainty.
        :param save_to: a path to save the resulting CSV file to.
        :param row_limit: place a limit on the number of samples / rows evaluated in the Minimal-Pair Dataset.
        :return: A DataFrame containing the evaluation results for each sample.
        """

        # --- Prepare dataset ---
        mp_dataset_iter = mp_dataset.itertuples()
        if row_limit:
            mp_dataset_iter = itertools.islice(mp_dataset_iter, row_limit)
        n = len(mp_dataset) if not row_limit else row_limit

        # --- Load model & tokenizer ---
        is_encoder = model_type == "encoder"
        results = []

        with self.evaluator.load_model(model_repo, is_encoder):
            # --- Evaluate each row ---
            for row in tqdm(mp_dataset_iter, total=n, desc="Evaluating"):
                row_results = _init_row_results(row)

                try:
                    if is_encoder:
                        self._evaluate_encoder_row(row, row_results, evaluation_cols)
                    else:
                        self._evaluate_decoder_row(row, row_results, evaluation_cols)

                except TooManyMasksException:
                    logging.error(f"Too many masks in {getattr(row, 'sentence_id')}")
                    continue
                except Exception as e:
                    raise RuntimeError(f"Model/tokeniser issue: {e}") from e

                # --- Entropy ---
                certainty_score = self.evaluator.get_entropy_based_certainty(k=entropy_topk, normalise=True)
            
                row_results["certainty"] = certainty_score

                results.append(row_results)

        # evaluation_columns = pd.Series(EVAL_TEMPLATE.keys())
        results_df = pd.DataFrame(results, columns=row_results.keys())
        self.evaluation_dataset = results_df

        if save_to is not None:
            results_df.to_csv(save_to, index=False)

        return results_df

    def evaluate_from_filepath(
        self,
        mp_dataset_filepath: str,
        model_repo: str,
        model_type: str,
        evaluation_cols:list[str]=["form_grammatical","form_ungrammatical"],
        entropy_topk: int = 100,
        save_to: str|None=None,
        row_limit: int|None = None,
    ) -> pd.DataFrame:
        """
        Carries out model evaluation using a Minimal-Pair Dataset generated by Grew-TSE that is provided as a filepath.

        :param mp_dataset_filepath: the filepath pointing to the Minimal-Pair Dataset. Should be a .csv format.
        :param model_repo: the Hugging Face model repository link.
        :param model_type: choose either 'encoder' or 'decoder'.
        :param entropy_topk: how many probabilities are taken into account for calculating the model uncertainty.
        :param save_to: a path to save the resulting CSV file to.
        :param row_limit: place a limit on the number of samples / rows evaluated in the Minimal-Pair Dataset.
        :return: A DataFrame containing the evaluation results for each sample.
        """
        mp_dataset = load_and_validate_mp_dataset(mp_dataset_filepath)
        return self.evaluate_model(
            mp_dataset, model_repo, model_type, evaluation_cols, entropy_topk, save_to, row_limit
        )

    def load_evaluation_results(self, evaluation_dataset_filepath: str) -> None:
        path = Path(evaluation_dataset_filepath)
        if not path.is_file():
            raise FileNotFoundError(f"Invalid evaluation file: {path}")

        try:
            df = pd.read_csv(path)
        except Exception as e:
            raise ValueError(f"Could not read evaluation dataset: {path}") from e

        if df.empty:
            raise ValueError("Evaluation dataset is empty")

        self.evaluation_dataset = df


    def _evaluate_encoder_row(self, row, row_results, evaluation_cols:list[str]=["form_grammatical","form_ungrammatical"]):
        try:
            targets = [getattr(row,c) for c in evaluation_cols]
            result_probs = self.evaluator.run_masked_prediction(
                row.masked_text,
                targets
            )

            for c,p in zip(evaluation_cols, result_probs):
                row_results[f"p_{c}"] = p
                row_results[f"I_{c}"] = compute_surprisal(p)

            if "ood_minimal_pairs" in row:
                self._evaluate_ood_pairs(
                    row,
                    row_results,
                    lambda g,u: self.evaluator.run_masked_prediction(
                        row.masked_text, [g,u]
                    ),
                )
        except Exception as e:
            raise RuntimeError(f"Failed to perform masked language modelling for row {getattr(row, 'sentence_id')}: {e}")


    def _evaluate_decoder_row(self, row, row_results, evaluation_cols:list[str]=["form_grammatical","form_ungrammatical"]):
        try:
            targets = [getattr(row,c) for c in evaluation_cols]
            result_probs = self.evaluator.run_next_word_prediction(
                row.prompt_text, 
                targets
            )

            for c,p in zip(evaluation_cols, result_probs):
                row_results[f"p_{c}"] = p
                row_results[f"I_{c}"] = compute_surprisal(p)

            if "ood_minimal_pairs" in row:
                self._evaluate_ood_pairs(
                    row,
                    row_results,
                    lambda g, u: self.evaluator.run_next_word_prediction(
                        row.prompt_text, [g,u]
                    ),
                )

        except Exception as e:
            raise RuntimeError(f"Failed to perform masked next-word prediction for row {getattr(row, 'sentence_id')}: {e}")

    def _evaluate_ood_pairs(self, row, row_results, evaluation_func):
        ood_pairs = ast.literal_eval(row.ood_pairs)
        all_ood_probs_gram = []
        all_ood_probs_ungram = []

        for pair in ood_pairs:
            prob_gram, prob_ungram = evaluation_func(pair[0], pair[1])
            all_ood_probs_gram.append(prob_gram)
            all_ood_probs_ungram.append(prob_ungram)

        avg_ood_prob_gram = compute_mean(all_ood_probs_gram)
        avg_ood_prob_ungram = compute_mean(all_ood_probs_ungram)

        row_results.update(
            {
                "ood_avg_p_grammatical": avg_ood_prob_gram,
                "ood_avg_p_ungrammatical": avg_ood_prob_ungram,
                "ood_avg_I_grammatical": compute_surprisal(avg_ood_prob_gram),
                "ood_avg_I_ungrammatical": compute_surprisal(avg_ood_prob_ungram),
            }
        )

    def get_avg_surprisal_difference(self, grammatical_column:str="p_grammatical", ungrammatical_column:str="p_ungrammatical") -> float:
        """
        Get the normalised average surprisal difference (ASD).
        A higher score indicates that the model, on average, tends towards being more confident in the grammatical word over the ungrammatical one.
        However, this is not quite fully accurate and as with any average ASD scores may suffer from outliers skewing the result.

        :return: the value of the normalised ASD.
        """
        if self.evaluation_dataset is None:
            raise KeyError("Please evaluate a model first or load evaluation results.")
        return compute_average_surprisal_difference(
            self.evaluation_dataset[grammatical_column],
            self.evaluation_dataset[ungrammatical_column],
        )

    def get_norm_avg_surprisal_difference(self, grammatical_column:str="p_grammatical",ungrammatical_column:str="p_ungrammatical") -> float:
        """
        Get the normalised average surprisal difference (ASD).
        A higher score indicates that the model, on average, tends towards being more confident in the grammatical word over the ungrammatical one.
        However, this is not quite fully accurate and as with any average ASD scores may suffer from outliers skewing the result.

        This normalised version simply calculates (Average Grammatical Surprisal - Average Ungrammatical Surprisal) / Average Grammatical Surprisal

        :return: the value of the normalised ASD.
        """
        if self.evaluation_dataset is None:
            raise KeyError("Please evaluate a model first or load evaluation results.")
        return compute_normalised_surprisal_difference(
            self.evaluation_dataset[grammatical_column],
            self.evaluation_dataset[ungrammatical_column],
        )

    def get_avg_certainty(self) -> float:
        if self.evaluation_dataset is None:
            raise KeyError("Please evaluate a model first or load evaluation results.")
        else:
            return round(compute_mean(self.evaluation_dataset["certainty"].to_list()),2)

    def get_accuracy(self, grammatical_column:str="p_grammatical",ungrammatical_column:str="p_ungrammatical") -> float:
        """
        Get the proportion of the time that the model predicts the grammatical form over the ungrammatical form.
        A value of -1 indicates something went wrong.

        :return: a float between 0 and 1, where 1 is 100% accuracy.
        """
        if isinstance(self._get_grammatical_form_probs(grammatical_column), pd.Series) and isinstance(self._get_ungrammatical_form_probs(ungrammatical_column), pd.Series):
            return compute_accuracy(self._get_grammatical_form_probs(grammatical_column), self._get_ungrammatical_form_probs(ungrammatical_column))
        else:
            return -1


    def _get_grammatical_form_probs(self, col:str="p_grammatical") -> pd.Series|None:
        if self.evaluation_dataset is not None and col in self.evaluation_dataset.columns:
            g_form_probs = self.evaluation_dataset[col]
            if isinstance(g_form_probs, pd.Series):
                return g_form_probs
        else:
            raise KeyError("Please evaluate a model first.")

    def _get_ungrammatical_form_probs(self, col:str="p_ungrammatical") -> pd.Series|None:
        if self.evaluation_dataset is not None and col in self.evaluation_dataset.columns:
            ug_form_probs = self.evaluation_dataset[col]
            if isinstance(ug_form_probs, pd.Series):
                return ug_form_probs
        else:
            raise KeyError("Please evaluate a model first.")


class Evaluator:
    def __init__(self):
        self.tokeniser: PreTrainedTokenizerBase|None = None
        self.model: PreTrainedModel|None = None

        self.mask_token_index: int = -1
        self.mask_probs: torch.Tensor | None = None
        self.logits: torch.Tensor|None = None
        self.device: str|None = None

    def setup_parameters(
        self, model_name: str, is_mlm: bool = True, device: str = "cpu"
    ) -> Tuple[PreTrainedModel, PreTrainedTokenizerBase]:
        self.tokeniser = AutoTokenizer.from_pretrained(model_name)
        if is_mlm:
            self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # set to eval mode, disabling things like dropout
        self.device = device
        self.model.to(device)
        self.model.eval()

        return self.model, self.tokeniser
    
    @contextmanager
    def load_model(self, model_name: str, is_mlm: bool = True):
        """Context manager for proper model cleanup."""
        try:
            self.setup_parameters(model_name, is_mlm)
            yield self
        finally:
            if self.model is not None:
                del self.model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            self.model = None
            self.tokeniser = None

    def run_masked_prediction(
            self, sentence: str, targets: list[str],
    ) -> list[float]:
        if not self.model or not self.tokeniser:
            raise RuntimeError("Model and tokenizer must be loaded before prediction.")

        mask_token = self.tokeniser.mask_token
        sentence_masked = sentence.replace("[MASK]", mask_token)

        if sentence_masked.count(mask_token) != 1:
            raise TooManyMasksException("Only single-mask sentences are supported.")

        masked_ids = self.tokeniser.encode(sentence_masked, add_special_tokens=False)
        mask_index = masked_ids.index(self.tokeniser.mask_token_id)

        device = next(self.model.parameters()).device

        probs = []
        for target_word in targets:
            t_ids = self.tokeniser.encode(target_word, add_special_tokens=False)
            t_prob = self._compute_masked_joint_probability(
                masked_ids, mask_index, t_ids, device
            )
            probs.append(t_prob)

        return probs

    def _compute_masked_joint_probability(
        self, input_ids: List[int], mask_index: int, word_ids: List[int], device
    ) -> float:
        input_ids_tensor = torch.tensor([input_ids], device=device)
        log_prob = 0.0
        index = mask_index

        for i, tid in enumerate(word_ids):
            with torch.no_grad():
                logits = self.model(input_ids_tensor).logits

            probs = F.softmax(logits[:, index, :], dim=-1)
            token_prob = max(probs[0, tid].item(), 1e-12)  # avoid log(0)
            log_prob += math.log(token_prob + 1e-12)

            if i == 0:
                self.mask_probs = probs

            # Replace mask with predicted token
            input_ids_tensor[0, index] = tid

            # Insert new mask if more tokens remain
            if i < len(word_ids) - 1:
                input_ids_tensor = torch.cat(
                    [
                        input_ids_tensor[:, : index + 1],
                        torch.tensor([[self.tokeniser.mask_token_id]], device=device),
                        input_ids_tensor[:, index + 1 :],
                    ],
                    dim=1,
                )

                index += 1

        return math.exp(log_prob)

    def run_next_word_prediction(
            self, context: str, targets: list[str]
        ) -> list[float]:
        if not self.model or not self.tokeniser:
            raise RuntimeError("Model and tokenizer must be loaded before prediction.")

        context_ids = self.tokeniser.encode(context, add_special_tokens=False)
        device = next(self.model.parameters()).device

        probs = []
        for t in targets:
            t_ids = self.tokeniser.encode(t, add_special_tokens=False)
            t_prob = self._compute_next_word_joint_probability(context_ids, t_ids, device)
            probs.append(t_prob)

        return probs

    def _compute_next_word_joint_probability(
        self, input_ids: List[int], word_ids: List[int], device
    ) -> float:
        input_ids_tensor = torch.tensor([input_ids], device=device)
        log_prob = 0.0

        for i, tid in enumerate(word_ids):
            with torch.no_grad():
                logits = self.model(input_ids_tensor).logits

            index = input_ids_tensor.shape[1] - 1  # last token position
            probs = F.softmax(logits[:, index, :], dim=-1)
            token_prob = probs[0, tid].item()
            log_prob += math.log(token_prob + 1e-12)

            if i == 0:
                self.mask_probs = probs

            # Append predicted token to context
            input_ids_tensor = torch.cat(
                [input_ids_tensor, torch.tensor([[tid]], device=device)], dim=1
            )

        return math.exp(log_prob)

    def get_entropy_based_certainty(self, k: int = 100, normalise: bool = False) -> float:
        """Compute an entropic certainty score over the prediction distribution.

        k: Number of top tokens to consider.
        normalise: Whether to normalise entropy.

        Returns:
        :returns: Certainty value based on entropy calculations over token probabiliity distribution.
        """
        if self.mask_probs is None:
            raise ValueError("No output probabilities available. Run evaluation first.")
        return compute_entropy_based_certainty(self.mask_probs[0], k)

    def _get_mask_index(self, inputs: Any) -> int:
        if "input_ids" not in inputs:
            raise ValueError("Missing 'input_ids' in inputs.")
        elif self.tokeniser.mask_token_id is None:
            raise ValueError("The tokeniser does not have a defined mask_token_id.")

        input_ids = inputs["input_ids"]
        mask_positions = torch.where(input_ids == self.tokeniser.mask_token_id)

        if len(mask_positions[0]) == 0:
            raise ValueError("No mask token found in input_ids.")
        elif len(mask_positions[0]) > 1:
            raise ValueError("Multiple mask tokens found; expected only one.")

        return (
            mask_positions[1].item()
            if len(mask_positions) > 1
            else mask_positions[0].item()
        )

    def _get_mask_probabilities(
        self, mask_token_index: int, logits: Any
    ) -> torch.Tensor:
        mask_logits = logits[0, mask_token_index, :]
        probs = F.softmax(mask_logits, dim=-1)  # shape: (vocab_size, )
        return probs
